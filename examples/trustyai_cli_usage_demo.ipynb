{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# TrustyAI CLI Usage Demo\n",
        "\n",
        "This notebook shows how to use the TrustyAI CLI for various AI safety operations including model evaluation, explanations, and fairness metrics.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have installed TrustyAI with CLI support:\n",
        "\n",
        "```bash\n",
        "pip install .\n",
        "```\n",
        "\n",
        "For full evaluation support:\n",
        "\n",
        "```bash\n",
        "pip install .[eval]\n",
        "```\n",
        "\n",
        "Or for all features:\n",
        "\n",
        "```bash\n",
        "pip install .[all]\n",
        "```\n",
        "\n",
        "The CLI tool `trustyai` should be available after installation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[3m                         Available Evaluation Providers                         \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mProvider Name  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDescription                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLocal Mode\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKubernetes Mode\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36mlm-eval-harness\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mLM Evaluation Harness for  \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m✓         \u001b[0m\u001b[33m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m✓              \u001b[0m\u001b[33m \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mlanguage model evaluation. \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mAutomatically delegates to \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mlocal or Kubernetes        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimplementation based on    \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mdeployment mode.           \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "└─────────────────┴─────────────────────────────┴────────────┴─────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# List available evaluation providers (should show \"lm-eval-harness\" instead of separate providers)\n",
        "!trustyai eval list-providers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provider: lm-eval-harness\n",
            "Execution mode: local\n",
            "Model: google/flan-t5-base\n",
            "Tasks: arc_easy\n",
            "Limit: 3\n",
            "\n",
            "--- Dry Run Mode - Validation Only ---\n",
            "✅ Configuration validated successfully\n",
            "Use without --dry-run to execute the evaluation\n"
          ]
        }
      ],
      "source": [
        "# Example: Local evaluation with the unified provider name\n",
        "!trustyai eval execute \\\n",
        "  --provider lm-eval-harness \\\n",
        "  --execution-mode local \\\n",
        "  --model \"google/flan-t5-base\" \\\n",
        "  --tasks \"arc_easy\" \\\n",
        "  --limit 3 \\\n",
        "  --dry-run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Basic CLI Information\n",
        "\n",
        "Let's start by exploring the basic CLI functionality and getting help information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: trustyai [OPTIONS] COMMAND [ARGS]...\n",
            "\n",
            "  TrustyAI CLI tool for Trustworthy AI operations.\n",
            "\n",
            "Options:\n",
            "  --version  Show the version and exit.\n",
            "  --help     Show this message and exit.\n",
            "\n",
            "Commands:\n",
            "  eval     Model evaluation commands.\n",
            "  info     Display information about TrustyAI.\n",
            "  metrics  Fairness and performance metrics commands.\n",
            "  model    Model management commands.\n"
          ]
        }
      ],
      "source": [
        "# Display the main CLI help\n",
        "!trustyai --help\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trustyai, version 2.0.0a1\n"
          ]
        }
      ],
      "source": [
        "# Check the version\n",
        "!trustyai --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrustyAI SDK version 2.0.0a1\n",
            "A Python SDK for trustworthy AI\n"
          ]
        }
      ],
      "source": [
        "# Get general information about TrustyAI\n",
        "!trustyai info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrustyAI SDK version 2.0.0a1\n",
            "A Python SDK for trustworthy AI\n",
            "\n",
            "Additional Information:\n",
            "  - Python package for explainable and fair AI\n",
            "  - Supports model explanations and fairness metrics\n",
            "\u001b[3m   Available Providers    \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━┳━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mProvider Name  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mType\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━╇━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36mlm-eval-harness\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32meval\u001b[0m\u001b[32m \u001b[0m│\n",
            "└─────────────────┴──────┘\n"
          ]
        }
      ],
      "source": [
        "# Get verbose information including available providers\n",
        "!trustyai info --verbose\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Model Evaluation Commands\n",
        "\n",
        "The CLI provides comprehensive model evaluation functionality with support for multiple providers and deployment modes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: trustyai eval [OPTIONS] COMMAND [ARGS]...\n",
            "\n",
            "  Model evaluation commands.\n",
            "\n",
            "Options:\n",
            "  --help  Show this message and exit.\n",
            "\n",
            "Commands:\n",
            "  execute         Execute model evaluation with specified provider and...\n",
            "  list-datasets   List available evaluation datasets for a provider.\n",
            "  list-metrics    List available evaluation metrics for a provider.\n",
            "  list-providers  List available evaluation providers.\n"
          ]
        }
      ],
      "source": [
        "# Get help for evaluation commands\n",
        "!trustyai eval --help\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[3m                         Available Evaluation Providers                         \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mProvider Name  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDescription                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLocal Mode\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKubernetes Mode\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36mlm-eval-harness\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mLM Evaluation Harness for  \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m✓         \u001b[0m\u001b[33m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m✓              \u001b[0m\u001b[33m \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mlanguage model evaluation. \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mAutomatically delegates to \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mlocal or Kubernetes        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimplementation based on    \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "│\u001b[36m                 \u001b[0m│\u001b[32m \u001b[0m\u001b[32mdeployment mode.           \u001b[0m\u001b[32m \u001b[0m│\u001b[33m            \u001b[0m│\u001b[33m                 \u001b[0m│\n",
            "└─────────────────┴─────────────────────────────┴────────────┴─────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# List available evaluation providers\n",
        "!trustyai eval list-providers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No evaluation provider available.\n",
            "Try installing optional dependencies: pip install trustyai[eval]\n"
          ]
        }
      ],
      "source": [
        "# List available datasets for a specific provider\n",
        "!trustyai eval list-datasets --provider lm-evaluation-harness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: LMEvalProvider.__init__() takes 1 positional argument but 2 were given\n"
          ]
        }
      ],
      "source": [
        "# List available metrics for a specific provider\n",
        "!trustyai eval list-metrics --provider lm-eval-harness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Unified Evaluation Command\n",
        "\n",
        "The `trustyai eval execute` command is the main evaluation command that supports both local and Kubernetes execution modes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: trustyai eval execute [OPTIONS]\n",
            "\n",
            "  Execute model evaluation with specified provider and execution mode.\n",
            "\n",
            "  This unified command supports both local and Kubernetes execution modes.\n",
            "\n",
            "  Examples:   # Local execution   trustyai eval execute --provider lm-eval-\n",
            "  harness --execution-mode local \\     --model \"hf/microsoft/DialoGPT-medium\"\n",
            "  --tasks \"hellaswag,arc_easy\" --limit 10      # Kubernetes execution\n",
            "  trustyai eval execute --provider lm-eval-harness --execution-mode kubernetes\n",
            "  \\     --model \"hf/microsoft/DialoGPT-medium\" --tasks \"hellaswag,arc_easy\" \\\n",
            "  --namespace trustyai-eval --cpu 4 --memory 8Gi        # RAGAS evaluation\n",
            "  with external dataset   trustyai eval execute --provider ragas --execution-\n",
            "  mode local \\     --model \"openai/gpt-4\" --tasks\n",
            "  \"faithfulness,answer_relevancy\" \\     --dataset \"data/rag_evaluation.json\"\n",
            "\n",
            "Options:\n",
            "  -p, --provider TEXT             Name of the evaluation provider  [required]\n",
            "  --execution-mode [local|kubernetes]\n",
            "                                  Execution mode (default: local)\n",
            "  --model TEXT                    Model identifier/path  [required]\n",
            "  --tasks TEXT                    Comma-separated list of evaluation tasks\n",
            "                                  [required]\n",
            "  -l, --limit INTEGER             Limit the number of examples to evaluate\n",
            "  --batch-size INTEGER            Batch size for evaluation\n",
            "  -o, --output TEXT               Path to output results file\n",
            "  -f, --format [json|csv]         Output format (default: json)\n",
            "  -n, --namespace TEXT            Kubernetes namespace (for kubernetes\n",
            "                                  execution mode)\n",
            "  --cpu TEXT                      CPU limit for Kubernetes execution\n",
            "  --memory TEXT                   Memory limit for Kubernetes execution\n",
            "  --image TEXT                    Container image for Kubernetes execution\n",
            "  --dataset TEXT                  Path to external dataset (if required by\n",
            "                                  provider)\n",
            "  --parameters TEXT               Additional provider-specific parameters as\n",
            "                                  JSON string\n",
            "  --dry-run                       Validate configuration without executing\n",
            "  --watch                         Watch Kubernetes job progress (kubernetes\n",
            "                                  mode only)\n",
            "  --force                         Force execution despite validation warnings\n",
            "  --help                          Show this message and exit.\n"
          ]
        }
      ],
      "source": [
        "# Get help for the execute command\n",
        "!trustyai eval execute --help\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Example 1: Local Evaluation with LM-Evaluation-Harness\n",
        "\n",
        "This example shows how to run a local evaluation using the LM-Evaluation-Harness provider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provider: lm-eval-harness\n",
            "Execution mode: local\n",
            "Model: google/flan-t5-base\n",
            "Tasks: arc_easy, hellaswag\n",
            "Limit: 5\n",
            "Batch size: 1\n",
            "\n",
            "🚀 Starting evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n",
            "2025-06-28:00:09:55 INFO     [models.huggingface:137] Using device 'cuda'\n",
            "2025-06-28:00:09:56 INFO     [models.huggingface:382] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2025-06-28:00:09:59 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-28:00:09:59 INFO     [evaluator:243] Using pre-initialized model\n",
            "2025-06-28:00:10:08 WARNING  [evaluator:309] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-28:00:10:08 WARNING  [evaluator:309] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-28:00:10:08 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
            "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 2215.69it/s]\n",
            "2025-06-28:00:10:08 INFO     [api.task:434] Building contexts for arc_easy on rank 0...\n",
            "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 1751.71it/s]\n",
            "2025-06-28:00:10:08 INFO     [evaluator:559] Running loglikelihood requests\n",
            "Running loglikelihood requests:   0%|                    | 0/40 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Running loglikelihood requests: 100%|███████████| 40/40 [00:03<00:00, 10.64it/s]\n",
            "Error: cannot access local variable 'json' where it is not associated with a value\n"
          ]
        }
      ],
      "source": [
        "# Example: Local evaluation with a small model and limited examples\n",
        "!trustyai eval execute \\\n",
        "  --provider lm-eval-harness \\\n",
        "  --execution-mode local \\\n",
        "  --model \"google/flan-t5-base\" \\\n",
        "  --tasks \"arc_easy,hellaswag\" \\\n",
        "  --limit 5 \\\n",
        "  --batch-size 1 \\\n",
        "  --output local_eval_results.json \\\n",
        "  --format json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Example 2: Dry Run Mode\n",
        "\n",
        "Use the `--dry-run` flag to validate your configuration without actually running the evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Evaluation provider 'lm-evaluation-harness' not found.\n",
            "Try installing optional dependencies: pip install trustyai[eval]\n",
            "Use 'trustyai eval list-providers' to see available providers.\n"
          ]
        }
      ],
      "source": [
        "# Dry run to validate configuration\n",
        "!trustyai eval execute \\\n",
        "  --provider lm-evaluation-harness \\\n",
        "  --execution-mode local \\\n",
        "  --model \"microsoft/DialoGPT-medium\" \\\n",
        "  --tasks \"hellaswag,arc_easy\" \\\n",
        "  --limit 10 \\\n",
        "  --dry-run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Kubernetes Deployment\n",
        "\n",
        "This example shows how to deploy an evaluation job to Kubernetes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provider: lm-eval-harness\n",
            "Execution mode: kubernetes\n",
            "Model: microsoft/DialoGPT-medium\n",
            "Tasks: hellaswag, arc_easy\n",
            "Limit: 50\n",
            "Namespace: trustyai-eval\n",
            "CPU: 4\n",
            "Memory: 8Gi\n",
            "\n",
            "--- Dry Run Mode - Validation Only ---\n",
            "✅ Configuration validated successfully\n",
            "Use without --dry-run to execute the evaluation\n"
          ]
        }
      ],
      "source": [
        "# Example: Kubernetes evaluation deployment\n",
        "!trustyai eval execute \\\n",
        "  --provider lm-eval-harness \\\n",
        "  --execution-mode kubernetes \\\n",
        "  --model \"microsoft/DialoGPT-medium\" \\\n",
        "  --tasks \"hellaswag,arc_easy\" \\\n",
        "  --namespace trustyai-eval \\\n",
        "  --cpu 4 \\\n",
        "  --memory 8Gi \\\n",
        "  --limit 50 \\\n",
        "  --dry-run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 4: Using Additional Parameters\n",
        "\n",
        "You can pass additional provider-specific parameters using the `--parameters` flag with JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provider: lm-eval-harness\n",
            "Execution mode: local\n",
            "Model: google/flan-t5-base\n",
            "Tasks: arc_easy\n",
            "Limit: 3\n",
            "\n",
            "--- Dry Run Mode - Validation Only ---\n",
            "✅ Configuration validated successfully\n",
            "Use without --dry-run to execute the evaluation\n"
          ]
        }
      ],
      "source": [
        "# Example: Using additional parameters\n",
        "!trustyai eval execute \\\n",
        "  --provider lm-eval-harness \\\n",
        "  --execution-mode local \\\n",
        "  --model \"google/flan-t5-base\" \\\n",
        "  --tasks \"arc_easy\" \\\n",
        "  --limit 3 \\\n",
        "  --parameters '{\"num_fewshot\": 0, \"device\": \"cpu\", \"use_cache\": false}' \\\n",
        "  --dry-run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Example 5: CSV Output Format\n",
        "\n",
        "You can save results in CSV format for easier analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provider: lm-eval-harness\n",
            "Execution mode: local\n",
            "Model: google/flan-t5-base\n",
            "Tasks: arc_easy\n",
            "Limit: 3\n",
            "\n",
            "--- Dry Run Mode - Validation Only ---\n",
            "✅ Configuration validated successfully\n",
            "Use without --dry-run to execute the evaluation\n"
          ]
        }
      ],
      "source": [
        "# Example: Save results in CSV format\n",
        "!trustyai eval execute \\\n",
        "  --provider lm-eval-harness \\\n",
        "  --execution-mode local \\\n",
        "  --model \"google/flan-t5-base\" \\\n",
        "  --tasks \"arc_easy\" \\\n",
        "  --limit 3 \\\n",
        "  --output eval_results.csv \\\n",
        "  --format csv \\\n",
        "  --dry-run\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Common CLI Patterns and Best Practices\n",
        "\n",
        "Here are some common patterns and best practices when using the TrustyAI CLI.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1. Always Use Dry Run First\n",
        "\n",
        "Before running expensive evaluations, always use `--dry-run` to validate your configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (274547920.py, line 2)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtrustyai eval execute --provider lm-evaluation-harness --model \"your-model\" --tasks \"task1,task2\" --dry-run\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# First, validate with dry run\n",
        "trustyai eval execute --provider lm-evaluation-harness --model \"your-model\" --tasks \"task1,task2\" --dry-run\n",
        "\n",
        "# Then run the actual evaluation\n",
        "trustyai eval execute --provider lm-evaluation-harness --model \"your-model\" --tasks \"task1,task2\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2. Start Small with Limits\n",
        "\n",
        "When testing new models or tasks, use `--limit` to run on a small subset first:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provider: lm-eval-harness\n",
            "Execution mode: local\n",
            "Model: new-model\n",
            "Tasks: hellaswag\n",
            "Limit: 5\n",
            "\n",
            "🚀 Starting evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n",
            "2025-06-28:00:13:15 INFO     [models.huggingface:137] Using device 'cuda'\n",
            "Error: Evaluation failed: new-model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Provider: lm-eval-harness\n",
            "Execution mode: local\n",
            "Model: new-model\n",
            "Tasks: hellaswag\n",
            "Limit: 100\n",
            "\n",
            "🚀 Starting evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n",
            "2025-06-28:00:13:18 INFO     [models.huggingface:137] Using device 'cuda'\n",
            "Error: Evaluation failed: new-model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        }
      ],
      "source": [
        "# Test with small limit first\n",
        "!trustyai eval execute --provider lm-eval-harness --model \"new-model\" --tasks \"hellaswag\" --limit 5\n",
        "\n",
        "# Scale up after confirming it works\n",
        "!trustyai eval execute --provider lm-eval-harness --model \"new-model\" --tasks \"hellaswag\" --limit 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3. Use Kubernetes for Long-Running Evaluations\n",
        "\n",
        "For production evaluations or large-scale tasks, use Kubernetes mode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For large evaluations, use Kubernetes\n",
        "trustyai eval execute \\\n",
        "  --provider lm-evaluation-harness \\\n",
        "  --execution-mode kubernetes \\\n",
        "  --model \"large-model\" \\\n",
        "  --tasks \"comprehensive_task_suite\" \\\n",
        "  --namespace trustyai-production \\\n",
        "  --cpu 8 \\\n",
        "  --memory 32Gi \\\n",
        "  --watch  # Monitor progress\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
