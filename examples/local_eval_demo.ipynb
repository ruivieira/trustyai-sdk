{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# TrustyAI Evaluation Demo\n",
        "\n",
        "This notebook demonstrates how to use the TrustyAI SDK's local evaluation functionality to evaluate language models using the LM Evaluation Harness.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have installed TrustyAI with evaluation support:\n",
        "\n",
        "```bash\n",
        "pip install .[eval]\n",
        "```\n",
        "\n",
        "Or for all features:\n",
        "\n",
        "```bash\n",
        "pip install .[all]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Basic Setup and Imports\n",
        "\n",
        "First, let's import the necessary modules and check what evaluation providers are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from trustyai import Providers\n",
        "from trustyai.core import DeploymentMode\n",
        "from trustyai.core.eval import EvaluationProviderConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explore the new Providers class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TrustyAI Providers Class ===\n",
            "\n",
            "Available provider types:\n",
            "  - bias_detection\n",
            "  - eval\n",
            "  - evaluation\n",
            "  - explainability\n",
            "\n",
            "Available evaluation providers:\n",
            "  - LMEvalProvider\n",
            "  - Lm-evalProvider\n",
            "\n",
            "We'll use: Providers.eval.LMEvalProvider\n",
            "The deployment mode in the config will determine whether it runs locally or on Kubernetes.\n"
          ]
        }
      ],
      "source": [
        "print(\"=== TrustyAI Providers Class ===\")\n",
        "print(\"\\nAvailable provider types:\")\n",
        "for provider_type in dir(Providers):\n",
        "    print(f\"  - {provider_type}\")\n",
        "\n",
        "print(f\"\\nAvailable evaluation providers:\")\n",
        "for provider_name in dir(Providers.eval):\n",
        "    print(f\"  - {provider_name}\")\n",
        "\n",
        "print(f\"\\nWe'll use: Providers.eval.LMEvalProvider\")\n",
        "print(\"The deployment mode in the config will determine whether it runs locally or on Kubernetes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Initialise the Local Evaluation Provider\n",
        "\n",
        "Now let's create and initialise the local evaluation provider using the new organised Providers class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rui/Sync/code/rh/trusty/trustyai-sdk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Evaluation provider initialised successfully!\n",
            "Provider class: LMEvalProvider\n",
            "Provider type: eval\n",
            "Supported deployment modes: ['local', 'kubernetes']\n"
          ]
        }
      ],
      "source": [
        "# Create the evaluation provider using the new Providers class\n",
        "# This automatically handles local vs Kubernetes deployment based on config\n",
        "provider = Providers.eval.LMEvalProvider()\n",
        "\n",
        "# Initialise the provider (this will check if lm-eval is available)\n",
        "try:\n",
        "    provider.initialize()\n",
        "    print(\"✓ Evaluation provider initialised successfully!\")\n",
        "    print(f\"Provider class: {provider.__class__.__name__}\")\n",
        "    print(f\"Provider type: {provider.get_provider_type()}\")\n",
        "    print(f\"Supported deployment modes: {[mode.value for mode in provider.get_supported_deployment_modes()]}\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Error initialising provider: {e}\")\n",
        "    print(\"Please install evaluation dependencies: pip install .[eval]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Explore Available Tasks and Metrics\n",
        "\n",
        "Let's see what evaluation tasks and metrics are available through the provider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of available tasks: 0\n",
            "\n",
            "First 10 available tasks:\n"
          ]
        }
      ],
      "source": [
        "# List available evaluation datasets/tasks\n",
        "available_tasks = provider.list_available_datasets()\n",
        "print(f\"Number of available tasks: {len(available_tasks)}\")\n",
        "print(\"\\nFirst 10 available tasks:\")\n",
        "for task in sorted(available_tasks)[:10]:\n",
        "    print(f\"  - {task}\")\n",
        "\n",
        "if len(available_tasks) > 10:\n",
        "    print(f\"  ... and {len(available_tasks) - 10} more tasks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available metrics (13):\n",
            "  - acc\n",
            "  - acc_norm\n",
            "  - perplexity\n",
            "  - bleu\n",
            "  - rouge\n",
            "  - exact_match\n",
            "  - f1\n",
            "  - precision\n",
            "  - recall\n",
            "  - matthews_correlation\n",
            "  - multiple_choice_grade\n",
            "  - wer\n",
            "  - ter\n"
          ]
        }
      ],
      "source": [
        "# List available metrics\n",
        "available_metrics = provider.list_available_metrics()\n",
        "print(f\"Available metrics ({len(available_metrics)}):\")\n",
        "for metric in available_metrics:\n",
        "    print(f\"  - {metric}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Basic Evaluation Example\n",
        "\n",
        "Let's run a basic evaluation using a small model and a simple task. We'll use google/flan-t5-base (a small model) and the HellaSwag task for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration created:\n",
            "  Model: google/flan-t5-base\n",
            "  Tasks: ['arc_easy']\n",
            "  Metrics: ['acc', 'acc_norm']\n",
            "  Device: cpu (should be 'cpu'!)\n",
            "  Limit: 5 examples\n",
            "  Batch size: 1\n",
            "  Few-shot examples: 0\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation configuration\n",
        "config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"arc_easy\",\n",
        "    model=\"google/flan-t5-base\",  # Small model for quick evaluation\n",
        "    tasks=[\"arc_easy\"],  # Common sense reasoning task\n",
        "    limit=5,  # Limit to 5 examples for quick demonstration\n",
        "    metrics=[\"acc\", \"acc_norm\"],  # Accuracy metrics\n",
        "    device=\"cpu\",  # Use CPU to avoid GPU requirements - this should now be respected!\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,  # Small batch size for stability\n",
        "    num_fewshot=0  # Zero-shot evaluation\n",
        ")\n",
        "\n",
        "print(\"Configuration created:\")\n",
        "print(f\"  Model: {config.model}\")\n",
        "print(f\"  Tasks: {config.tasks}\")\n",
        "print(f\"  Metrics: {config.metrics}\")\n",
        "print(f\"  Device: {config.device} (should be 'cpu'!)\")\n",
        "print(f\"  Limit: {config.limit} examples\")\n",
        "print(f\"  Batch size: {config.get_param('batch_size')}\")\n",
        "print(f\"  Few-shot examples: {config.get_param('num_fewshot')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the evaluation\n",
        "\n",
        "This may take a few minutes as the model needs to be downloaded and loaded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:51:46,480 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:51:46,899 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-16:20:51:52,213 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:20:51:52,214 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:20:51:55,521 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-16:20:51:55,521 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 5/5 [00:00<00:00, 2560.00it/s]\n",
            "2025-06-16:20:51:55,525 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 20/20 [00:02<00:00,  9.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Evaluation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    results = provider.evaluate(config)\n",
        "    print(\"\\n✓ Evaluation completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Evaluation failed: {e}\")\n",
        "    results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Deployment Mode Demonstration\n",
        "\n",
        "The advantage of using `Providers.eval.LMEvalProvider` is that the same provider can handle both local and Kubernetes deployments. The deployment mode is specified in the configuration, not in the provider selection.\n",
        "\n",
        "Let's demonstrate this by showing how you would configure for different deployment modes (we'll only run the local one for this demo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📍 LOCAL Configuration:\n",
            "  Deployment mode: local\n",
            "  Device: cpu\n",
            "\n",
            "🚀 KUBERNETES Configuration:\n",
            "  Deployment mode: kubernetes\n",
            "  Model: google/flan-t5-base\n",
            "\n",
            "✨ Same provider handles both: LMEvalProvider\n",
            "The provider automatically delegates based on deployment_mode!\n"
          ]
        }
      ],
      "source": [
        "# Same provider, different deployment modes!\n",
        "\n",
        "# Configuration for LOCAL deployment\n",
        "local_config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"local_demo\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tasks=[\"arc_easy\"],\n",
        "    limit=3,\n",
        "    metrics=[\"acc\"],\n",
        "    deployment_mode=DeploymentMode.LOCAL,  # This determines the implementation\n",
        "    device=\"cpu\",\n",
        "    batch_size=1,\n",
        "    num_fewshot=0\n",
        ")\n",
        "\n",
        "# Configuration for KUBERNETES deployment (same provider!)\n",
        "kubernetes_config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"kubernetes_demo\", \n",
        "    model=\"google/flan-t5-base\",\n",
        "    tasks=[\"arc_easy\"],\n",
        "    limit=3,\n",
        "    metrics=[\"acc\"],\n",
        "    deployment_mode=DeploymentMode.KUBERNETES,  # Different deployment mode\n",
        "    # Kubernetes-specific parameters would go here\n",
        "    # namespace=\"trustyai\",\n",
        "    # cpu=\"1000m\",\n",
        "    # memory=\"2Gi\"\n",
        ")\n",
        "\n",
        "print(\"📍 LOCAL Configuration:\")\n",
        "print(f\"  Deployment mode: {local_config.deployment_mode.value}\")\n",
        "print(f\"  Device: {local_config.device}\")\n",
        "\n",
        "print(\"\\n🚀 KUBERNETES Configuration:\")  \n",
        "print(f\"  Deployment mode: {kubernetes_config.deployment_mode.value}\")\n",
        "print(f\"  Model: {kubernetes_config.model}\")\n",
        "\n",
        "print(f\"\\n✨ Same provider handles both: {provider.__class__.__name__}\")\n",
        "print(\"The provider automatically delegates based on deployment_mode!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "==================\n",
            "\n",
            "Task: arc_easy\n",
            "--------------\n",
            "  alias: arc_easy\n",
            "  acc,none: 0.6000\n",
            "  acc_stderr,none: 0.2449\n",
            "  acc_norm,none: 0.6000\n",
            "  acc_norm_stderr,none: 0.2449\n"
          ]
        }
      ],
      "source": [
        "# Display results if evaluation was successful\n",
        "if results:\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(\"==================\")\n",
        "    \n",
        "    # Pretty print the results\n",
        "    if 'results' in results:\n",
        "        for task_name, task_results in results['results'].items():\n",
        "            print(f\"\\nTask: {task_name}\")\n",
        "            print(\"-\" * (len(task_name) + 6))\n",
        "            for metric, value in task_results.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "                else:\n",
        "                    print(f\"  {metric}: {value}\")\n",
        "    else:\n",
        "        print(\"Raw results:\")\n",
        "        pprint(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Multi-Task Evaluation\n",
        "\n",
        "Let's run an evaluation on multiple tasks to see how the model performs across different capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-task evaluation configuration:\n",
            "  Tasks: ['hellaswag', 'arc_easy', 'winogrande']\n",
            "  Limit per task: 3 examples\n"
          ]
        }
      ],
      "source": [
        "# Configuration for multi-task evaluation\n",
        "multi_task_config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"multi_task_demo\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tasks=[\n",
        "        \"hellaswag\",    # Common sense reasoning\n",
        "        \"arc_easy\",     # Science questions (easy)\n",
        "        \"winogrande\"    # Pronoun resolution\n",
        "    ],\n",
        "    limit=3,  # Very small limit for quick demo\n",
        "    metrics=[\"acc\", \"acc_norm\"],\n",
        "    device=\"cpu\",\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,\n",
        "    num_fewshot=0\n",
        ")\n",
        "\n",
        "print(\"Multi-task evaluation configuration:\")\n",
        "print(f\"  Tasks: {multi_task_config.tasks}\")\n",
        "print(f\"  Limit per task: {multi_task_config.limit} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:53:28,946 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running multi-task evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:53:29,219 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-16:20:53:34,535 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:20:53:34,536 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:20:53:43,573 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of winogrande from None to 0\n",
            "2025-06-16:20:53:43,573 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-16:20:53:43,574 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-16:20:53:43,608 INFO     [lm_eval.api.task:420] Building contexts for winogrande on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 14043.43it/s]\n",
            "2025-06-16:20:53:43,612 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 960.38it/s]\n",
            "2025-06-16:20:53:43,618 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 4233.82it/s]\n",
            "2025-06-16:20:53:43,624 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 30/30 [00:01<00:00, 16.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Multi-task evaluation completed!\n",
            "\n",
            "Results Summary:\n",
            "================\n",
            "\n",
            "ARC_EASY:\n",
            "  acc,none: 0.6667\n",
            "  acc_stderr,none: 0.3333\n",
            "  acc_norm,none: 0.6667\n",
            "  acc_norm_stderr,none: 0.3333\n",
            "\n",
            "HELLASWAG:\n",
            "  acc,none: 0.3333\n",
            "  acc_stderr,none: 0.3333\n",
            "  acc_norm,none: 0.3333\n",
            "  acc_norm_stderr,none: 0.3333\n",
            "\n",
            "WINOGRANDE:\n",
            "  acc,none: 0.6667\n",
            "  acc_stderr,none: 0.3333\n"
          ]
        }
      ],
      "source": [
        "# Run multi-task evaluation\n",
        "print(\"Running multi-task evaluation...\")\n",
        "\n",
        "try:\n",
        "    multi_results = provider.evaluate(multi_task_config)\n",
        "    print(\"\\n✓ Multi-task evaluation completed!\")\n",
        "    \n",
        "    # Display results for each task\n",
        "    if 'results' in multi_results:\n",
        "        print(\"\\nResults Summary:\")\n",
        "        print(\"================\")\n",
        "        \n",
        "        for task_name, task_results in multi_results['results'].items():\n",
        "            print(f\"\\n{task_name.upper()}:\")\n",
        "            for metric, value in task_results.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "                    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Multi-task evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Few-Shot Evaluation\n",
        "\n",
        "Let's demonstrate few-shot evaluation, where we provide examples to the model before asking it to perform the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot evaluation configuration:\n",
            "  Task: hellaswag\n",
            "  Few-shot examples: 2\n",
            "  Test examples: 3\n"
          ]
        }
      ],
      "source": [
        "# Configuration for few-shot evaluation\n",
        "few_shot_config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"few_shot_demo\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tasks=[\"hellaswag\"],\n",
        "    limit=3,\n",
        "    metrics=[\"acc\", \"acc_norm\"],\n",
        "    device=\"cpu\",\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,\n",
        "    num_fewshot=2  # Provide 2 examples before each test question\n",
        ")\n",
        "\n",
        "print(\"Few-shot evaluation configuration:\")\n",
        "print(f\"  Task: {few_shot_config.tasks[0]}\")\n",
        "print(f\"  Few-shot examples: {few_shot_config.get_param('num_fewshot')}\")\n",
        "print(f\"  Test examples: {few_shot_config.limit}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:53:59,666 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running few-shot evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:53:59,935 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-16:20:54:05,676 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:20:54:05,676 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:20:54:08,879 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 2\n",
            "2025-06-16:20:54:08,879 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 1266.65it/s]\n",
            "2025-06-16:20:54:08,885 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 12/12 [00:01<00:00,  9.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Few-shot evaluation completed!\n",
            "\n",
            "Few-shot Results for hellaswag:\n",
            "================================\n",
            "  acc,none: 0.3333\n",
            "  acc_stderr,none: 0.3333\n",
            "  acc_norm,none: 0.3333\n",
            "  acc_norm_stderr,none: 0.3333\n"
          ]
        }
      ],
      "source": [
        "# Run few-shot evaluation\n",
        "print(\"Running few-shot evaluation...\")\n",
        "\n",
        "try:\n",
        "    few_shot_results = provider.evaluate(few_shot_config)\n",
        "    print(\"\\n✓ Few-shot evaluation completed!\")\n",
        "    \n",
        "    # Display results\n",
        "    if 'results' in few_shot_results:\n",
        "        task_name = list(few_shot_results['results'].keys())[0]\n",
        "        task_results = few_shot_results['results'][task_name]\n",
        "        \n",
        "        print(f\"\\nFew-shot Results for {task_name}:\")\n",
        "        print(\"=\" * (len(task_name) + 23))\n",
        "        \n",
        "        for metric, value in task_results.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Few-shot evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Comparing Models\n",
        "\n",
        "Let's compare the performance of different models on the same task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:54:16,668 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating google/flan-t5-base...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:54:16,954 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-16:20:54:22,399 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:20:54:22,400 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:20:54:25,544 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-16:20:54:25,544 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 4800.81it/s]\n",
            "2025-06-16:20:54:25,548 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 18.98it/s]\n",
            "2025-06-16:20:54:26,663 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ google/flan-t5-base evaluation completed\n",
            "\n",
            "Evaluating google/flan-t5-small...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:20:54:27,045 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-16:20:54:33,320 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:20:54:33,320 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:20:54:36,278 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-16:20:54:36,279 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 4463.61it/s]\n",
            "2025-06-16:20:54:36,283 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 12/12 [00:04<00:00,  2.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ google/flan-t5-small evaluation completed\n"
          ]
        }
      ],
      "source": [
        "# List of models to compare (using small models for quick evaluation)\n",
        "models_to_compare = [\n",
        "    \"google/flan-t5-base\",\n",
        "    \"google/flan-t5-small\"\n",
        "]\n",
        "\n",
        "comparison_results = {}\n",
        "\n",
        "for model in models_to_compare:\n",
        "    print(f\"\\nEvaluating {model}...\")\n",
        "    \n",
        "    config = EvaluationProviderConfig(\n",
        "        evaluation_name=f\"comparison_{model}\",\n",
        "        model=model,\n",
        "        tasks=[\"hellaswag\"],\n",
        "        limit=3,\n",
        "        metrics=[\"acc\", \"acc_norm\"],\n",
        "        device=\"cpu\",\n",
        "        deployment_mode=DeploymentMode.LOCAL,\n",
        "        batch_size=1,\n",
        "        num_fewshot=0\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        results = provider.evaluate(config)\n",
        "        if 'results' in results:\n",
        "            task_results = results['results']['hellaswag']\n",
        "            comparison_results[model] = task_results\n",
        "            print(f\"  ✓ {model} evaluation completed\")\n",
        "        else:\n",
        "            print(f\"  ✗ {model} evaluation returned unexpected format\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ {model} evaluation failed: {e}\")\n",
        "        comparison_results[model] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Comparison Results:\n",
            "========================\n",
            "Model           Accuracy   Acc (Norm)\n",
            "-----------------------------------\n",
            "google/flan-t5-base N/A        N/A       \n",
            "google/flan-t5-small N/A        N/A       \n"
          ]
        }
      ],
      "source": [
        "# Display comparison results\n",
        "print(\"\\nModel Comparison Results:\")\n",
        "print(\"========================\")\n",
        "print(f\"{'Model':<15} {'Accuracy':<10} {'Acc (Norm)':<10}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for model, results in comparison_results.items():\n",
        "    if results:\n",
        "        acc = results.get('acc', 'N/A')\n",
        "        acc_norm = results.get('acc_norm', 'N/A')\n",
        "        \n",
        "        acc_str = f\"{acc:.4f}\" if isinstance(acc, (int, float)) else str(acc)\n",
        "        acc_norm_str = f\"{acc_norm:.4f}\" if isinstance(acc_norm, (int, float)) else str(acc_norm)\n",
        "        \n",
        "        print(f\"{model:<15} {acc_str:<10} {acc_norm_str:<10}\")\n",
        "    else:\n",
        "        print(f\"{model:<15} {'Failed':<10} {'Failed':<10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
