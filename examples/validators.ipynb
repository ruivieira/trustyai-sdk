{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# TrustyAI SDK Validators Demo\n",
    "\n",
    "This notebook demonstrates how to use the TrustyAI SDK's validators functionality.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have installed the TrustyAI SDK:\n",
    "\n",
    "```bash\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Basic Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and check what evaluation providers are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustyai import Providers\n",
    "from trustyai.core import DeploymentMode\n",
    "from trustyai.core.eval import EvaluationProviderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation provider\n",
    "provider = Providers.eval.LMEvalProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LMEvalProvider'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provider.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eval'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provider.provider_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported deployment modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local', 'kubernetes']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mode.value for mode in provider.supported_deployment_modes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Kubernetes deployment\n",
    "\n",
    "Let's create the configuration for an LMEval Kubernetes evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared configuration for both deployments\n",
    "shared_config = {\n",
    "    \"evaluation_name\": \"comparison_demo\",\n",
    "    \"model\": \"google/flan-t5-base\",\n",
    "    \"tasks\": [\"arc_easy\"],\n",
    "    \"limit\": 3,  # Small limit for quick comparison\n",
    "    \"metrics\": [\"acc\", \"acc_norm\"],\n",
    "    \"batch_size\": 1,\n",
    "    \"num_fewshot\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for KUBERNETES deployment\n",
    "kubernetes_config = EvaluationProviderConfig(\n",
    "    **shared_config,\n",
    "    deployment_mode=DeploymentMode.KUBERNETES,\n",
    "    namespace=\"test\",\n",
    "    deploy=True,\n",
    "    wait_for_completion=True,\n",
    "    timeout=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run same evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG - _parse_args_to_config] Args=1: has namespace? True\n",
      "[DEBUG - _parse_args_to_config] Namespace value: test\n",
      "[DEBUG - _evaluate_kubernetes_async] Config keys: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'additional_params', 'deployment_mode', 'device', 'evaluation_name', 'get_param', 'limit', 'metrics', 'model', 'tasks']\n",
      "[DEBUG - _evaluate_kubernetes_async] Config namespace: test\n",
      "üîç Running Kubernetes environment validation...\n",
      "‚ùå TrustyAI operator deployment not found in any namespace\n",
      "   üí° Suggestion: Install TrustyAI operator with documentation: https://trustyai.org/docs/main/trustyai-operator\n",
      "‚ùå Validation failed. Please fix the issues above before proceeding.\n"
     ]
    }
   ],
   "source": [
    "kubernetes_results = await provider.evaluate(kubernetes_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the TrustyAI operator is not installed on the cluster.\n",
    "Let's now install the TrustyAI operator and try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG - _parse_args_to_config] Args=1: has namespace? True\n",
      "[DEBUG - _parse_args_to_config] Namespace value: test\n",
      "[DEBUG - _evaluate_kubernetes_async] Config keys: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'additional_params', 'deployment_mode', 'device', 'evaluation_name', 'get_param', 'limit', 'metrics', 'model', 'tasks']\n",
      "[DEBUG - _evaluate_kubernetes_async] Config namespace: test\n",
      "üîç Running Kubernetes environment validation...\n",
      "‚úÖ TrustyAI operator is deployed and ready in namespace 'system'\n",
      "[DEBUG] Using namespace for CR: test\n",
      "[DEBUG] Setting limit in config as string: 3\n",
      "[DEBUG] Setting namespace in LMEvalJob resource: test\n",
      "[DEBUG] Setting limit as string: 3\n",
      "[DEBUG] Deploying LMEvalJob to namespace: test\n",
      "[DEBUG] Namespace 'test' does not exist. Please create it first with: kubectl create namespace test\n"
     ]
    }
   ],
   "source": [
    "kubernetes_results = await provider.evaluate(kubernetes_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, we forgot the `test` namespace. Let's create it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/test created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create namespace test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG - _parse_args_to_config] Args=1: has namespace? True\n",
      "[DEBUG - _parse_args_to_config] Namespace value: test\n",
      "[DEBUG - _evaluate_kubernetes_async] Config keys: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'additional_params', 'deployment_mode', 'device', 'evaluation_name', 'get_param', 'limit', 'metrics', 'model', 'tasks']\n",
      "[DEBUG - _evaluate_kubernetes_async] Config namespace: test\n",
      "üîç Running Kubernetes environment validation...\n",
      "‚úÖ TrustyAI operator is deployed and ready in namespace 'system'\n",
      "[DEBUG] Using namespace for CR: test\n",
      "[DEBUG] Setting limit in config as string: 3\n",
      "[DEBUG] Setting namespace in LMEvalJob resource: test\n",
      "[DEBUG] Setting limit as string: 3\n",
      "[DEBUG] Deploying LMEvalJob to namespace: test\n",
      "[DEBUG] API Group: trustyai.opendatahub.io, Version: v1alpha1\n",
      "[DEBUG] Resource metadata: {'name': 'evaljob-92b7b4b9', 'namespace': 'test'}\n",
      "[DEBUG] Successfully created LMEvalJob 'evaljob-92b7b4b9' in namespace 'test'\n",
      "[DEBUG] Successfully deployed LMEvalJob: evaljob-92b7b4b9\n",
      "[DEBUG] Waiting for completion of job: evaljob-92b7b4b9\n",
      "‚è≥ Waiting for job evaljob-92b7b4b9 to complete...\n",
      "[DEBUG] Job evaljob-92b7b4b9 state: complete\n",
      "[DEBUG] Job completed but no results found\n"
     ]
    }
   ],
   "source": [
    "kubernetes_results = await provider.evaluate(kubernetes_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
