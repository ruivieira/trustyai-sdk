{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# TrustyAI Local Evaluation Demo\n",
        "\n",
        "This notebook demonstrates how to use the TrustyAI SDK's local evaluation functionality to evaluate language models using the LM Evaluation Harness.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have installed TrustyAI with evaluation support:\n",
        "\n",
        "```bash\n",
        "pip install .[eval]\n",
        "```\n",
        "\n",
        "Or for all features:\n",
        "\n",
        "```bash\n",
        "pip install .[all]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Basic Setup and Imports\n",
        "\n",
        "First, let's import the necessary modules and check what evaluation providers are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "from trustyai.core.eval import EvaluationProviderConfig\n",
        "from trustyai.core import DeploymentMode\n",
        "from trustyai.providers.eval import LocalLMEvalProvider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Initialise the Local Evaluation Provider\n",
        "\n",
        "Let's create and initialise the local evaluation provider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rui/Sync/code/rh/trusty/trustyai-sdk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Local evaluation provider initialised successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create the local evaluation provider\n",
        "provider = LocalLMEvalProvider()\n",
        "\n",
        "# Initialise the provider (this will check if lm-eval is available)\n",
        "try:\n",
        "    provider.initialize()\n",
        "    print(\"✓ Local evaluation provider initialised successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Error initialising provider: {e}\")\n",
        "    print(\"Please install evaluation dependencies: pip install .[eval]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Explore Available Tasks and Metrics\n",
        "\n",
        "Let's see what evaluation tasks and metrics are available through the provider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of available tasks: 0\n",
            "\n",
            "First 10 available tasks:\n"
          ]
        }
      ],
      "source": [
        "# List available evaluation datasets/tasks\n",
        "available_tasks = provider.list_available_datasets()\n",
        "print(f\"Number of available tasks: {len(available_tasks)}\")\n",
        "print(\"\\nFirst 10 available tasks:\")\n",
        "for task in sorted(available_tasks)[:10]:\n",
        "    print(f\"  - {task}\")\n",
        "\n",
        "if len(available_tasks) > 10:\n",
        "    print(f\"  ... and {len(available_tasks) - 10} more tasks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available metrics (13):\n",
            "  - acc\n",
            "  - acc_norm\n",
            "  - perplexity\n",
            "  - bleu\n",
            "  - rouge\n",
            "  - exact_match\n",
            "  - f1\n",
            "  - precision\n",
            "  - recall\n",
            "  - matthews_correlation\n",
            "  - multiple_choice_grade\n",
            "  - wer\n",
            "  - ter\n"
          ]
        }
      ],
      "source": [
        "# List available metrics\n",
        "available_metrics = provider.list_available_metrics()\n",
        "print(f\"Available metrics ({len(available_metrics)}):\")\n",
        "for metric in available_metrics:\n",
        "    print(f\"  - {metric}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Basic Evaluation Example\n",
        "\n",
        "Let's run a basic evaluation using a small model and a simple task. We'll use google/flan-t5-base (a small model) and the HellaSwag task for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration created:\n",
            "  Model: google/flan-t5-base\n",
            "  Tasks: ['arc_easy']\n",
            "  Metrics: ['acc', 'acc_norm']\n",
            "  Device: cpu\n",
            "  Limit: 5 examples\n",
            "  Batch size: 1\n",
            "  Few-shot examples: 0\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation configuration\n",
        "config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"arc_easy\",\n",
        "    model=\"google/flan-t5-base\",  # Small model for quick evaluation\n",
        "    tasks=[\"arc_easy\"],  # Common sense reasoning task\n",
        "    limit=5,  # Limit to 5 examples for quick demonstration\n",
        "    metrics=[\"acc\", \"acc_norm\"],  # Accuracy metrics\n",
        "    device=\"cpu\",  # Use CPU to avoid GPU requirements\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,  # Small batch size for stability\n",
        "    num_fewshot=0  # Zero-shot evaluation\n",
        ")\n",
        "\n",
        "print(\"Configuration created:\")\n",
        "print(f\"  Model: {config.model}\")\n",
        "print(f\"  Tasks: {config.tasks}\")\n",
        "print(f\"  Metrics: {config.metrics}\")\n",
        "print(f\"  Device: {config.device}\")\n",
        "print(f\"  Limit: {config.limit} examples\")\n",
        "print(f\"  Batch size: {config.get_param('batch_size')}\")\n",
        "print(f\"  Few-shot examples: {config.get_param('num_fewshot')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:05:44,249 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n",
            "This may take a few minutes as the model needs to be downloaded and loaded.\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:05:44,737 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2025-06-16:01:05:50,241 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:01:05:50,241 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:01:05:53,581 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-16:01:05:53,581 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 5/5 [00:00<00:00, 2561.56it/s]\n",
            "2025-06-16:01:05:53,585 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 20/20 [00:01<00:00, 12.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Evaluation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation\n",
        "print(\"Running evaluation...\")\n",
        "print(\"This may take a few minutes as the model needs to be downloaded and loaded.\")\n",
        "\n",
        "try:\n",
        "    results = provider.evaluate(config)\n",
        "    print(\"\\n✓ Evaluation completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Evaluation failed: {e}\")\n",
        "    results = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "==================\n",
            "\n",
            "Task: arc_easy\n",
            "--------------\n",
            "  alias: arc_easy\n",
            "  acc,none: 0.6000\n",
            "  acc_stderr,none: 0.2449\n",
            "  acc_norm,none: 0.6000\n",
            "  acc_norm_stderr,none: 0.2449\n"
          ]
        }
      ],
      "source": [
        "# Display results if evaluation was successful\n",
        "if results:\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(\"==================\")\n",
        "    \n",
        "    # Pretty print the results\n",
        "    if 'results' in results:\n",
        "        for task_name, task_results in results['results'].items():\n",
        "            print(f\"\\nTask: {task_name}\")\n",
        "            print(\"-\" * (len(task_name) + 6))\n",
        "            for metric, value in task_results.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "                else:\n",
        "                    print(f\"  {metric}: {value}\")\n",
        "    else:\n",
        "        print(\"Raw results:\")\n",
        "        pprint(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Multi-Task Evaluation\n",
        "\n",
        "Let's run an evaluation on multiple tasks to see how the model performs across different capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-task evaluation configuration:\n",
            "  Tasks: ['hellaswag', 'arc_easy', 'winogrande']\n",
            "  Limit per task: 3 examples\n"
          ]
        }
      ],
      "source": [
        "# Configuration for multi-task evaluation\n",
        "multi_task_config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"multi_task_demo\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tasks=[\n",
        "        \"hellaswag\",    # Common sense reasoning\n",
        "        \"arc_easy\",     # Science questions (easy)\n",
        "        \"winogrande\"    # Pronoun resolution\n",
        "    ],\n",
        "    limit=3,  # Very small limit for quick demo\n",
        "    metrics=[\"acc\", \"acc_norm\"],\n",
        "    device=\"cpu\",\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,\n",
        "    num_fewshot=0\n",
        ")\n",
        "\n",
        "print(\"Multi-task evaluation configuration:\")\n",
        "print(f\"  Tasks: {multi_task_config.tasks}\")\n",
        "print(f\"  Limit per task: {multi_task_config.limit} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:06:44,210 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running multi-task evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:06:44,480 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2025-06-16:01:06:50,014 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:01:06:50,014 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "Downloading data: 100%|██████████| 3.40M/3.40M [00:00<00:00, 5.05MB/s]\n",
            "Generating train split: 100%|██████████| 40398/40398 [00:00<00:00, 119801.92 examples/s]\n",
            "Generating test split: 100%|██████████| 1767/1767 [00:00<00:00, 117351.52 examples/s]\n",
            "Generating validation split: 100%|██████████| 1267/1267 [00:00<00:00, 115648.91 examples/s]\n",
            "2025-06-16:01:07:01,530 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of winogrande from None to 0\n",
            "2025-06-16:01:07:01,530 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-16:01:07:01,530 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-16:01:07:01,531 INFO     [lm_eval.api.task:420] Building contexts for winogrande on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 42366.71it/s]\n",
            "2025-06-16:01:07:01,532 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 2243.74it/s]\n",
            "2025-06-16:01:07:01,534 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 5347.60it/s]\n",
            "2025-06-16:01:07:01,537 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 30/30 [00:02<00:00, 12.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Multi-task evaluation completed!\n",
            "\n",
            "Results Summary:\n",
            "================\n",
            "\n",
            "ARC_EASY:\n",
            "  acc,none: 0.6667\n",
            "  acc_stderr,none: 0.3333\n",
            "  acc_norm,none: 0.6667\n",
            "  acc_norm_stderr,none: 0.3333\n",
            "\n",
            "HELLASWAG:\n",
            "  acc,none: 0.3333\n",
            "  acc_stderr,none: 0.3333\n",
            "  acc_norm,none: 0.3333\n",
            "  acc_norm_stderr,none: 0.3333\n",
            "\n",
            "WINOGRANDE:\n",
            "  acc,none: 0.6667\n",
            "  acc_stderr,none: 0.3333\n"
          ]
        }
      ],
      "source": [
        "# Run multi-task evaluation\n",
        "print(\"Running multi-task evaluation...\")\n",
        "\n",
        "try:\n",
        "    multi_results = provider.evaluate(multi_task_config)\n",
        "    print(\"\\n✓ Multi-task evaluation completed!\")\n",
        "    \n",
        "    # Display results for each task\n",
        "    if 'results' in multi_results:\n",
        "        print(\"\\nResults Summary:\")\n",
        "        print(\"================\")\n",
        "        \n",
        "        for task_name, task_results in multi_results['results'].items():\n",
        "            print(f\"\\n{task_name.upper()}:\")\n",
        "            for metric, value in task_results.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "                    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Multi-task evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Few-Shot Evaluation\n",
        "\n",
        "Let's demonstrate few-shot evaluation, where we provide examples to the model before asking it to perform the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot evaluation configuration:\n",
            "  Task: hellaswag\n",
            "  Few-shot examples: 2\n",
            "  Test examples: 3\n"
          ]
        }
      ],
      "source": [
        "# Configuration for few-shot evaluation\n",
        "few_shot_config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"few_shot_demo\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tasks=[\"hellaswag\"],\n",
        "    limit=3,\n",
        "    metrics=[\"acc\", \"acc_norm\"],\n",
        "    device=\"cpu\",\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,\n",
        "    num_fewshot=2  # Provide 2 examples before each test question\n",
        ")\n",
        "\n",
        "print(\"Few-shot evaluation configuration:\")\n",
        "print(f\"  Task: {few_shot_config.tasks[0]}\")\n",
        "print(f\"  Few-shot examples: {few_shot_config.get_param('num_fewshot')}\")\n",
        "print(f\"  Test examples: {few_shot_config.limit}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:07:32,786 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running few-shot evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:07:33,051 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2025-06-16:01:07:38,612 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:01:07:38,613 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:01:07:41,843 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 2\n",
            "2025-06-16:01:07:41,844 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 1334.21it/s]\n",
            "2025-06-16:01:07:41,849 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 12/12 [00:01<00:00,  6.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Few-shot evaluation completed!\n",
            "\n",
            "Few-shot Results for hellaswag:\n",
            "================================\n",
            "  acc,none: 0.3333\n",
            "  acc_stderr,none: 0.3333\n",
            "  acc_norm,none: 0.3333\n",
            "  acc_norm_stderr,none: 0.3333\n"
          ]
        }
      ],
      "source": [
        "# Run few-shot evaluation\n",
        "print(\"Running few-shot evaluation...\")\n",
        "\n",
        "try:\n",
        "    few_shot_results = provider.evaluate(few_shot_config)\n",
        "    print(\"\\n✓ Few-shot evaluation completed!\")\n",
        "    \n",
        "    # Display results\n",
        "    if 'results' in few_shot_results:\n",
        "        task_name = list(few_shot_results['results'].keys())[0]\n",
        "        task_results = few_shot_results['results'][task_name]\n",
        "        \n",
        "        print(f\"\\nFew-shot Results for {task_name}:\")\n",
        "        print(\"=\" * (len(task_name) + 23))\n",
        "        \n",
        "        for metric, value in task_results.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Few-shot evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Comparing Models\n",
        "\n",
        "Let's compare the performance of different models on the same task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:13:28,193 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating google/flan-t5-base...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:13:28,546 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2025-06-16:01:13:34,134 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:01:13:34,134 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:01:13:37,453 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-16:01:13:37,454 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 5504.34it/s]\n",
            "2025-06-16:01:13:37,457 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 12/12 [00:01<00:00, 11.88it/s]\n",
            "2025-06-16:01:13:38,992 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ google/flan-t5-base evaluation completed\n",
            "\n",
            "Evaluating google/flan-t5-small...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cuda for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-16:01:13:42,164 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2025-06-16:01:14:07,510 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-16:01:14:07,510 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-16:01:14:10,545 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-06-16:01:14:10,545 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 3/3 [00:00<00:00, 5673.09it/s]\n",
            "2025-06-16:01:14:10,549 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 43.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ google/flan-t5-small evaluation completed\n"
          ]
        }
      ],
      "source": [
        "# List of models to compare (using small models for quick evaluation)\n",
        "models_to_compare = [\n",
        "    \"google/flan-t5-base\",\n",
        "    \"google/flan-t5-small\"\n",
        "]\n",
        "\n",
        "comparison_results = {}\n",
        "\n",
        "for model in models_to_compare:\n",
        "    print(f\"\\nEvaluating {model}...\")\n",
        "    \n",
        "    config = EvaluationProviderConfig(\n",
        "        evaluation_name=f\"comparison_{model}\",\n",
        "        model=model,\n",
        "        tasks=[\"hellaswag\"],\n",
        "        limit=3,\n",
        "        metrics=[\"acc\", \"acc_norm\"],\n",
        "        device=\"cpu\",\n",
        "        deployment_mode=DeploymentMode.LOCAL,\n",
        "        batch_size=1,\n",
        "        num_fewshot=0\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        results = provider.evaluate(config)\n",
        "        if 'results' in results:\n",
        "            task_results = results['results']['hellaswag']\n",
        "            comparison_results[model] = task_results\n",
        "            print(f\"  ✓ {model} evaluation completed\")\n",
        "        else:\n",
        "            print(f\"  ✗ {model} evaluation returned unexpected format\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ {model} evaluation failed: {e}\")\n",
        "        comparison_results[model] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Comparison Results:\n",
            "========================\n",
            "Model           Accuracy   Acc (Norm)\n",
            "-----------------------------------\n",
            "google/flan-t5-base N/A        N/A       \n",
            "google/flan-t5-small N/A        N/A       \n"
          ]
        }
      ],
      "source": [
        "# Display comparison results\n",
        "print(\"\\nModel Comparison Results:\")\n",
        "print(\"========================\")\n",
        "print(f\"{'Model':<15} {'Accuracy':<10} {'Acc (Norm)':<10}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for model, results in comparison_results.items():\n",
        "    if results:\n",
        "        acc = results.get('acc', 'N/A')\n",
        "        acc_norm = results.get('acc_norm', 'N/A')\n",
        "        \n",
        "        acc_str = f\"{acc:.4f}\" if isinstance(acc, (int, float)) else str(acc)\n",
        "        acc_norm_str = f\"{acc_norm:.4f}\" if isinstance(acc_norm, (int, float)) else str(acc_norm)\n",
        "        \n",
        "        print(f\"{model:<15} {acc_str:<10} {acc_norm_str:<10}\")\n",
        "    else:\n",
        "        print(f\"{model:<15} {'Failed':<10} {'Failed':<10}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
